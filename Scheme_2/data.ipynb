{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\n",
    "dataset=open('split_dataset','rb')\n",
    "dataset=pickle.load(dataset)\n",
    "testset=open('split_testset','rb')\n",
    "testset=pickle.load(testset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def label_get(dataset):\n",
    "    labelset=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        labelset.append(dataset.loc[index,'label'])\n",
    "    return labelset\n",
    "\n",
    "labelset=label_get(dataset)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def pd_li_transform(dataset):\n",
    "    dataset_list=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        str2=\" \"\n",
    "        if dataset.loc[index,'comment_all']is None:\n",
    "            dataset_list.append(dataset.loc[index,'content'])\n",
    "        else:\n",
    "            dataset_list.append(dataset.loc[index,'content']+dataset.loc[index,'comment_all'])\n",
    "    return dataset_list\n",
    "testset_list=pd_li_transform(testset)\n",
    "dataset_list=pd_li_transform(dataset)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def list_deal(dataset,labelset):\n",
    "    labelset2=[]\n",
    "    dataset2=[]\n",
    "    for i in range(len(dataset)):\n",
    "        if (dataset[i]!=[]):\n",
    "            dataset2.append(dataset[i])\n",
    "            labelset2.append(labelset[i])\n",
    "    return dataset2,labelset2\n",
    "dataset_list,labelset=list_deal(dataset_list,labelset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def list_onehot(labelset):\n",
    "    newli=[]\n",
    "    for index in labelset:\n",
    "        if (index==-1):newli.append([-1.0,0.0,0.0])\n",
    "        if (index==0):newli.append([0.0,-1.0,0.0])\n",
    "        if (index==1):newli.append([0.0,0.0,-1.0])\n",
    "    return newli\n",
    "def list_float(labelset):\n",
    "    newli=[]\n",
    "    for index in labelset:\n",
    "        if (index==-1):newli.append(0.0)\n",
    "        if (index==0):newli.append(1.0)\n",
    "        if (index==1):newli.append(2.0)\n",
    "    return newli\n",
    "labelset3=list_float(labelset)\n",
    "labelset2=list_onehot(labelset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def li_str_transform(dataset_list):\n",
    "    dataset2=[]\n",
    "    for seq in dataset_list:\n",
    "        str1=' '.join(seq)\n",
    "        dataset2.append(str1)\n",
    "    return dataset2\n",
    "dataset_str=li_str_transform(dataset_list)\n",
    "testset_str=li_str_transform(testset_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "from fastNLP import DataSet\n",
    "data_init=DataSet({'raw_chars':dataset_list,'target':labelset2})\n",
    "data_train=data_init[:35000]\n",
    "data_validation=data_init[35001:]\n",
    "data_test=DataSet({'raw_chars':testset_list})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "from fastNLP import Vocabulary\n",
    "vocab=Vocabulary()\n",
    "vocab.from_dataset(data_train,field_name='raw_chars',no_create_entry_dataset=[data_validation])\n",
    "vocab.index_dataset(data_train,field_name='raw_chars')\n",
    "vocab.index_dataset(data_validation,field_name='raw_chars')\n",
    "vocab.index_dataset(data_test,field_name='raw_chars')\n",
    "#target_vocab=Vocabulary(padding=None,unknown=None)\n",
    "#target_vocab.from_dataset(data_train,field_name='target')\n",
    "#target_vocab.index_dataset(data_train,field_name='target')\n",
    "#arget_vocab.index_dataset(data_validation,field_name='target')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Vocabulary(['王嘉尔', '热血', '街', '舞团', '或']...)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "data_train\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+------------------------------------------+--------+\n",
       "| raw_chars                                | target |\n",
       "+------------------------------------------+--------+\n",
       "| [3721, 3675, 1813, 7477, 3721, 352, 3... | 1.0    |\n",
       "| [97, 2, 974, 149, 32, 98, 167, 59, 18... | 0.0    |\n",
       "| [15642, 5691, 6, 9217, 7075, 2, 154, ... | 1.0    |\n",
       "| [536, 114, 167, 2, 127, 182, 35, 709,... | 0.0    |\n",
       "| [10, 21211, 2, 7642, 22, 2, 8062, 226... | 1.0    |\n",
       "| [18670, 83, 19401, 492, 378, 141, 506... | 2.0    |\n",
       "| [3652, 3620, 2607, 554, 225, 12, 5346... | 2.0    |\n",
       "| [145, 220, 3, 5, 47, 134, 2, 15173, 3... | 2.0    |\n",
       "| [270, 1200, 77, 39, 1692, 24843, 8876... | 2.0    |\n",
       "| [1441, 4317, 129, 4, 7, 175, 5, 274, ... | 1.0    |\n",
       "| [7643, 2, 268, 10394, 2, 7171, 15174,... | 1.0    |\n",
       "| [88769, 1641, 16692, 2, 121, 45732, 1... | 1.0    |\n",
       "| ...                                      | ...    |\n",
       "+------------------------------------------+--------+"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "data_train.rename_field('raw_chars','words')\n",
    "data_validation.rename_field('raw_chars','words')\n",
    "data_train.set_input('words')\n",
    "data_train.set_target('target')\n",
    "data_validation.set_input('words')\n",
    "data_validation.set_target('target')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "+------------------+------------------------------------------+\n",
       "| target           | words                                    |\n",
       "+------------------+------------------------------------------+\n",
       "| [-1.0, 0.0, 0.0] | [143, 6, 1071, 73, 240, 700, 328, 441... |\n",
       "| [0.0, 0.0, -1.0] | [11692, 251, 39496, 2749, 59995, 34, ... |\n",
       "| [0.0, 0.0, -1.0] | [158503, 1188, 16448, 25969, 420, 2, ... |\n",
       "| [0.0, -1.0, 0.0] | [4159, 236, 518, 2, 153, 81620, 2, 69... |\n",
       "| [0.0, 0.0, -1.0] | [1093, 1586, 230, 199, 2424, 3228, 27... |\n",
       "| [0.0, 0.0, -1.0] | [683, 2707, 46321, 9399, 3, 51, 3, 54... |\n",
       "| [-1.0, 0.0, 0.0] | [616, 64, 40, 229, 3, 794, 67, 1265, ... |\n",
       "| [0.0, 0.0, -1.0] | [328, 104, 1339, 3783, 1148, 2496, 31... |\n",
       "| [0.0, -1.0, 0.0] | [9841, 10021, 15083, 14882, 19515, 98... |\n",
       "| [0.0, 0.0, -1.0] | [5048, 368, 126, 19264, 626, 109, 209... |\n",
       "| [0.0, 0.0, -1.0] | [574, 3553, 120, 280, 128, 960, 1225,... |\n",
       "| [0.0, -1.0, 0.0] | [7836, 32, 5018, 921, 167, 1516, 135,... |\n",
       "| ...              | ...                                      |\n",
       "+------------------+------------------------------------------+"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "print(data_validation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+------------------------------------------+\n",
      "| target           | words                                    |\n",
      "+------------------+------------------------------------------+\n",
      "| [-1.0, 0.0, 0.0] | [143, 6, 1071, 73, 240, 700, 328, 441... |\n",
      "| [0.0, 0.0, -1.0] | [11692, 251, 39496, 2749, 59995, 34, ... |\n",
      "| [0.0, 0.0, -1.0] | [158503, 1188, 16448, 25969, 420, 2, ... |\n",
      "| [0.0, -1.0, 0.0] | [4159, 236, 518, 2, 153, 81620, 2, 69... |\n",
      "| [0.0, 0.0, -1.0] | [1093, 1586, 230, 199, 2424, 3228, 27... |\n",
      "| [0.0, 0.0, -1.0] | [683, 2707, 46321, 9399, 3, 51, 3, 54... |\n",
      "| [-1.0, 0.0, 0.0] | [616, 64, 40, 229, 3, 794, 67, 1265, ... |\n",
      "| [0.0, 0.0, -1.0] | [328, 104, 1339, 3783, 1148, 2496, 31... |\n",
      "| [0.0, -1.0, 0.0] | [9841, 10021, 15083, 14882, 19515, 98... |\n",
      "| [0.0, 0.0, -1.0] | [5048, 368, 126, 19264, 626, 109, 209... |\n",
      "| [0.0, 0.0, -1.0] | [574, 3553, 120, 280, 128, 960, 1225,... |\n",
      "| [0.0, -1.0, 0.0] | [7836, 32, 5018, 921, 167, 1516, 135,... |\n",
      "| ...              | ...                                      |\n",
      "+------------------+------------------------------------------+\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import torch\n",
    "from fastNLP.embeddings import BertEmbedding\n",
    "from fastNLP import Vocabulary\n",
    "from fastNLP.core.losses import MSELoss\n",
    "embed=BertEmbedding(vocab,model_dir_or_name='cn-wwm', include_cls_sep=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading vocabulary file /Users/wumozhou/.fastNLP/embedding/bert-chinese-wwm/vocab.txt\n",
      "Load pre-trained BERT parameters from file /Users/wumozhou/.fastNLP/embedding/bert-chinese-wwm/chinese_wwm_pytorch.bin.\n",
      "Bert Model will return 1 layers (layer-0 is embedding result): [-1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "from fastNLP.models import BertForSequenceClassification\n",
    "model = BertForSequenceClassification(embed,num_labels=3)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/34/5ls3qb4j0rl0kbsw8j22d1_w0000gn/T/ipykernel_59373/541041081.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBertForSequenceClassification\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membed\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/models/bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, words)\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0;34m:\u001B[0m\u001B[0;32mreturn\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m{\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0mattr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mfastNLP\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mConst\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOUTPUT\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mlogits\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_labels\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m         \"\"\"\n\u001B[0;32m---> 77\u001B[0;31m         \u001B[0mhidden\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     78\u001B[0m         \u001B[0mcls_hidden\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls_hidden\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/embeddings/bert_embedding.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, words)\u001B[0m\n\u001B[1;32m    136\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0moutputs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwords\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    139\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/embeddings/bert_embedding.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, words)\u001B[0m\n\u001B[1;32m    452\u001B[0m         \"\"\"\n\u001B[1;32m    453\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 454\u001B[0;31m             \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_word_len\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwords\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    455\u001B[0m             \u001B[0mword_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwords\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mne\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_word_pad_index\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# 为1的地方有word\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m             \u001B[0mseq_len\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword_mask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from fastNLP import Trainer, CrossEntropyLoss, AccuracyMetric, Adam\n",
    "\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(data_train, model,\n",
    "                  optimizer=Adam(model_params=model.parameters(), lr=2e-5),\n",
    "                  loss=CrossEntropyLoss(), device=device,\n",
    "                  batch_size=8, dev_data=data_validation,\n",
    "                  metrics=AccuracyMetric(), n_epochs=2, print_every=1)\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\twords: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 247]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.float32, (3)shape:torch.Size([2, 3]) \n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "In AccuracyMetric.evaluate(self, pred, target, seq_len=None), when pred have size:torch.Size([2]), target should have size: torch.Size([2]) or torch.Size([]), got torch.Size([2, 3]).",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/34/5ls3qb4j0rl0kbsw8j22d1_w0000gn/T/ipykernel_59373/609788098.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBertForSequenceClassification\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membed\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnum_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mdevice\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m'cpu'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m trainer = Trainer(data_train, model,\n\u001B[0m\u001B[1;32m      6\u001B[0m                   \u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mAdam\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_params\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2e-5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m                   \u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mCrossEntropyLoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/trainer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, train_data, model, optimizer, loss, batch_size, sampler, drop_last, update_every, num_workers, n_epochs, print_every, dev_data, metrics, metric_key, validate_every, save_path, use_tqdm, device, callbacks, check_code_level, fp16, **kwargs)\u001B[0m\n\u001B[1;32m    557\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    558\u001B[0m                     \u001B[0mcheck_batch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_batch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 559\u001B[0;31m             _check_code(dataset=train_data, model=self.model, losser=losser, forward_func=self._forward_func, metrics=metrics,\n\u001B[0m\u001B[1;32m    560\u001B[0m                         \u001B[0mdev_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdev_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric_key\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetric_key\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_level\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheck_code_level\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    561\u001B[0m                         batch_size=check_batch_size)\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/trainer.py\u001B[0m in \u001B[0;36m_check_code\u001B[0;34m(dataset, model, losser, metrics, forward_func, batch_size, dev_data, metric_key, check_level)\u001B[0m\n\u001B[1;32m   1012\u001B[0m         tester = Tester(data=dev_data[:batch_size * DEFAULT_CHECK_NUM_BATCH], model=model, metrics=metrics,\n\u001B[1;32m   1013\u001B[0m                         batch_size=batch_size, verbose=-1, use_tqdm=False)\n\u001B[0;32m-> 1014\u001B[0;31m         \u001B[0mevaluate_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtester\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1015\u001B[0m         \u001B[0m_check_eval_results\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mevaluate_results\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric_key\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetric_key\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1016\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/tester.py\u001B[0m in \u001B[0;36mtest\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    191\u001B[0m                                                 f\"must be `dict`, got {type(pred_dict)}.\")\n\u001B[1;32m    192\u001B[0m                             \u001B[0;32mfor\u001B[0m \u001B[0mmetric\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 193\u001B[0;31m                                 \u001B[0mmetric\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    194\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_tqdm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/metrics.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, pred_dict, target_dict)\u001B[0m\n\u001B[1;32m    275\u001B[0m         \u001B[0mrefined_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mmapped_pred_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mmapped_target_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 277\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mrefined_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    278\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.8/site-packages/fastNLP/core/metrics.py\u001B[0m in \u001B[0;36mevaluate\u001B[0;34m(self, pred, target, seq_len)\u001B[0m\n\u001B[1;32m    459\u001B[0m                 \u001B[0mwarnings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"You are not passing `seq_len` to exclude pad when calculate accuracy.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    460\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 461\u001B[0;31m             raise RuntimeError(f\"In {_get_func_signature(self.evaluate)}, when pred have \"\n\u001B[0m\u001B[1;32m    462\u001B[0m                                \u001B[0;34mf\"size:{pred.size()}, target should have size: {pred.size()} or \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    463\u001B[0m                                f\"{pred.size()[:-1]}, got {target.size()}.\")\n",
      "\u001B[0;31mRuntimeError\u001B[0m: In AccuracyMetric.evaluate(self, pred, target, seq_len=None), when pred have size:torch.Size([2]), target should have size: torch.Size([2]) or torch.Size([]), got torch.Size([2, 3])."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from torch.testing._internal.distributed.rpc.examples.parameter_server_test import batch_size\n",
    "from fastNLP import DataSetIter\n",
    "\n",
    "DataSetIter(dataset, batch_size=batch_size, sampler=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word1=torch.LongTensor(vocab.to_index(\"男\"))\n",
    "word2=torch.LongTensor(vocab.to_index(\"女\"))\n",
    "word3=torch.LongTensor(vocab.to_index(\"热血\"))\n",
    "#word1=embed(word1)\n",
    "#word2=embed(word2)\n",
    "#word3=embed(word3)\n",
    "print(word1.size())\n",
    "print(word2.size())\n",
    "print(word3.size())\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([475])\n",
      "torch.Size([333])\n",
      "torch.Size([3675])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "7dd44356432448c335aab49bb4a35eec45732cd176bb87a60d07cce9b4b0faba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}